## Data

The dataset used to train the predictor is the EnronSent Corpus:

Styler, Will (2011). The EnronSent Corpus. Technical Report 01-2011, University of Colorado at Boulder Institute of Cognitive Science, Boulder, CO.

More information can be found in the paper and [here](http://wstyler.ucsd.edu/enronsent.html). The dataset should be extracted and placed in `data/raw/`.


## Preprocessing
  - [NLP Text Preprocessing: A Practical Guide and Template](https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79)
  - [Build a simple predictive keyboard using python and keras](https://medium.com/analytics-vidhya/build-a-simple-predictive-keyboard-using-python-and-keras-b78d3c88cffb)
  - [Next word prediction](https://juan0001.github.io/next-word-prediction/)

## Models

###### General examples:
 - [Google Smartcompose: Using Neural Networks to Compose Emails](https://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html)
 - [Google Tensorflow: Neural Machine Translation With Attention](https://www.tensorflow.org/tutorials/text/nmt_with_attention)
 

###### Specific models:
- N-gram models:
  - Bickel, S., Haider, P., & Scheffer, T. (2005, October). Learning to complete sentences. In European Conference on Machine Learning (pp. 497-504). Springer, Berlin, Heidelberg.
- RNN-LM:
  - [Text generation with an RNN](https://www.tensorflow.org/tutorials/text/text_generation)
- Bag of Words (BoW):
  - ..


<small>Note: current list of references is general corpus of relevant material. This will be trimmed when finalized.</small>
